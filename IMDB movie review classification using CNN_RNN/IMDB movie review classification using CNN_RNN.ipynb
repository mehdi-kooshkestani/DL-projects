{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 class=\"devsite-page-title\"><span style=\"color: #ffff99;\">tf.keras.preprocessing.sequence.pad_sequences</span></h2>\n",
    "<ul>\n",
    "<li>\n",
    "<pre class=\"prettyprint language-python language-python\" data-language=\"python\"><span style=\"color: #ffffff;\">tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>sequence<span class=\"token punctuation\">.</span>pad_sequences<span class=\"token punctuation\">(</span>\n",
    "    sequences<span class=\"token punctuation\">,</span>\n",
    "    maxlen<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span>\n",
    "    dtype<span class=\"token operator\">=</span><span class=\"token string\">'int32'</span><span class=\"token punctuation\">,</span>\n",
    "    padding<span class=\"token operator\">=</span><span class=\"token string\">'pre'</span><span class=\"token punctuation\">,</span>\n",
    "    truncating<span class=\"token operator\">=</span><span class=\"token string\">'pre'</span><span class=\"token punctuation\">,</span>\n",
    "    value<span class=\"token operator\">=</span><span class=\"token number\">0.0</span>\n",
    "<span class=\"token punctuation\">)</span></span></pre>\n",
    "<ul>\n",
    "<li>\n",
    "<p><span style=\"color: #ccffff;\">Pads sequences to the same length.</span></p>\n",
    "<p><span style=\"color: #ccffff;\">This function transforms a list of&nbsp;<code>num_samples</code>&nbsp;sequences (lists of integers) into a 2D Numpy array of shape&nbsp;<code>(num_samples, num_timesteps)</code>.&nbsp;<code>num_timesteps</code>&nbsp;is either the&nbsp;<code>maxlen</code>&nbsp;argument if provided, or the length of the longest sequence otherwise.</span></p>\n",
    "<p><span style=\"color: #ccffff;\">Sequences that are shorter than&nbsp;<code>num_timesteps</code>&nbsp;are padded with&nbsp;<code>value</code>&nbsp;at the end.</span></p>\n",
    "<p><span style=\"color: #ccffff;\">Sequences longer than&nbsp;<code>num_timesteps</code>&nbsp;are truncated so that they fit the desired length. The position where padding or truncation happens is determined by the arguments&nbsp;<code>padding</code>&nbsp;and&nbsp;<code>truncating</code>, respectively.</span></p>\n",
    "<p><span style=\"color: #ccffff;\">Pre-padding is the default.</span></p>\n",
    "</li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>sequences</code></strong>: List of lists, where each element is a sequence.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>maxlen</code></strong>: Int, maximum length of all sequences.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>dtype</code></strong>: Type of the output sequences.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>padding</code></strong>: String, 'pre' or 'post': pad either before or after each sequence.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>truncating</code></strong>: String, 'pre' or 'post': remove values from sequences larger than&nbsp;<code>maxlen</code>, either at the beginning or at the end of the sequences.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>value</code></strong>: Float, padding value.</span></li>\n",
    "<li>\n",
    "<p>&nbsp;</p>\n",
    "<h4 id=\"returns\"><span style=\"color: #ff9900;\">Returns:</span></h4>\n",
    "<ul>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>x</code></strong>: Numpy array with shape&nbsp;</span><code><span style=\"color: #ffff99;\">(len(sequences), maxlen)</span></code>\n",
    "<h4 id=\"returns\">&nbsp;</h4>\n",
    "<pre class=\"prettyprint language-python language-python\" data-language=\"python\">&nbsp;</pre>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Padding sequences (samples x time)\n",
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence #note that from keras.preprocessing import sequence gives an error,tensorflow should be put before that\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "imdb_path='D:\\CSV original\\imdb.npz'\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=imdb_path,num_words=max_features)    # Loads the data as lists of integers\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Padding sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import callbacks\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color: #ff9900;\">multi layer GRU</span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 500, 32)           15552     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 500, 32)           6336      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,308,257\n",
      "Trainable params: 1,308,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 242s 1s/step - loss: 0.6370 - acc: 0.7334 - val_loss: 0.4215 - val_acc: 0.8576\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 236s 1s/step - loss: 0.4195 - acc: 0.8684 - val_loss: 0.4215 - val_acc: 0.8578\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 237s 1s/step - loss: 0.3581 - acc: 0.8965 - val_loss: 0.4295 - val_acc: 0.8627\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 245s 1s/step - loss: 0.3294 - acc: 0.9088 - val_loss: 0.4227 - val_acc: 0.8634\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 242s 1s/step - loss: 0.3161 - acc: 0.9176 - val_loss: 0.3877 - val_acc: 0.8790\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 241s 1s/step - loss: 0.2926 - acc: 0.9257 - val_loss: 0.3972 - val_acc: 0.8790\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 242s 1s/step - loss: 0.2733 - acc: 0.9326 - val_loss: 0.3841 - val_acc: 0.8807\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 242s 1s/step - loss: 0.2631 - acc: 0.9405 - val_loss: 0.4124 - val_acc: 0.8830\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 250s 1s/step - loss: 0.2428 - acc: 0.9459 - val_loss: 0.4198 - val_acc: 0.8801\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 252s 1s/step - loss: 0.2351 - acc: 0.9499 - val_loss: 0.4369 - val_acc: 0.8809\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len, embeddings_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(layers.GRU(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.GRU(32, return_sequences=True))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks.TensorBoard(log_dir='logs/RNN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><span style=\"color: #ffff99;\">embedding layer parameters no: 10000(words) * 128 (features)=1280000</span></li>\n",
    "<li><span style=\"color: #ffff99;\">GRU: 3*(32*32+32*128+2*32)=15552</span></li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color: #ff9900;\">1D convnet</span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 492, 32)           36896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 492, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 246, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 238, 32)           9248      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 238, 32)           0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,326,177\n",
      "Trainable params: 1,326,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 105s 523ms/step - loss: 0.7761 - acc: 0.6640 - val_loss: 0.6210 - val_acc: 0.8288\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 98s 502ms/step - loss: 0.4645 - acc: 0.8546 - val_loss: 0.5601 - val_acc: 0.8620\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 98s 500ms/step - loss: 0.4223 - acc: 0.8795 - val_loss: 0.5412 - val_acc: 0.8695\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 97s 497ms/step - loss: 0.4002 - acc: 0.8907 - val_loss: 0.5270 - val_acc: 0.8694\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 97s 495ms/step - loss: 0.3900 - acc: 0.8946 - val_loss: 0.5138 - val_acc: 0.8686\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 97s 497ms/step - loss: 0.3770 - acc: 0.9053 - val_loss: 0.5238 - val_acc: 0.8677\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 97s 495ms/step - loss: 0.3684 - acc: 0.9095 - val_loss: 0.5267 - val_acc: 0.8599\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 97s 495ms/step - loss: 0.3693 - acc: 0.9114 - val_loss: 0.5264 - val_acc: 0.8520\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 98s 498ms/step - loss: 0.3529 - acc: 0.9190 - val_loss: 0.5099 - val_acc: 0.8684\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 99s 507ms/step - loss: 0.3556 - acc: 0.9214 - val_loss: 0.5119 - val_acc: 0.8668\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len, embeddings_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(layers.Conv1D(32, 9, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(32, 9, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks.TensorBoard(log_dir='logs/CNN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style=\"color: #ff9900;\">1D convnet + GRU</span></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 492, 32)           36896     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 492, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 246, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 238, 32)           9248      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 238, 32)           0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,332,513\n",
      "Trainable params: 1,332,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 130s 651ms/step - loss: 0.6904 - acc: 0.6849 - val_loss: 0.5293 - val_acc: 0.8102\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 123s 627ms/step - loss: 0.4920 - acc: 0.8304 - val_loss: 0.4096 - val_acc: 0.8766\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 124s 635ms/step - loss: 0.3842 - acc: 0.8943 - val_loss: 0.3890 - val_acc: 0.8840\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 123s 629ms/step - loss: 0.3523 - acc: 0.9083 - val_loss: 0.3941 - val_acc: 0.8871\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 123s 628ms/step - loss: 0.3236 - acc: 0.9244 - val_loss: 0.4191 - val_acc: 0.8800\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 122s 625ms/step - loss: 0.3165 - acc: 0.9302 - val_loss: 0.4236 - val_acc: 0.8784\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 123s 626ms/step - loss: 0.2969 - acc: 0.9406 - val_loss: 0.4322 - val_acc: 0.8873\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 123s 627ms/step - loss: 0.2838 - acc: 0.9472 - val_loss: 0.4886 - val_acc: 0.8698\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 123s 626ms/step - loss: 0.2777 - acc: 0.9531 - val_loss: 0.4668 - val_acc: 0.8810\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 123s 628ms/step - loss: 0.2658 - acc: 0.9595 - val_loss: 0.4626 - val_acc: 0.8782\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len, embeddings_regularizer=regularizers.l2(1e-3)))\n",
    "model.add(layers.Conv1D(32, 9, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(32, 9, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks.TensorBoard(log_dir='logs/CNN_RNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11364), started 0:09:56 ago. (Use '!kill 11364' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7337d7eaf52e4f3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7337d7eaf52e4f3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
