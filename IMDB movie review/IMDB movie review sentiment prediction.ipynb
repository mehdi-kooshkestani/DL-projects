{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 class=\"devsite-page-title\"><span style=\"color: #ffff99;\">tf.keras.datasets.imdb.load_data:</span></h3>\n",
    "<ul>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>path</code></strong>: where to cache the data (relative to&nbsp;<code>~/.keras/dataset</code>).</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>num_words</code></strong>: max number of words to include. Words are ranked by how often they occur (in the training set) and only the most frequent words are kept</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>maxlen</code></strong>: sequences longer than this will be filtered out.</span></li>\n",
    "<li><span style=\"color: #ffff99;\"><strong><code>seed</code></strong>: random seed for sample shuffling</span></li>\n",
    "<li>\n",
    "<p><span style=\"color: #ffff99;\"><strong>returnes</strong>:Tuple of Numpy arrays:&nbsp;<code>(x_train, y_train), (x_test, y_test)</code>.</span></p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2]),\n",
       "       list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 5000\n",
    "# Cuts off the text after this number of words (among the max_features most common words)\n",
    "max_len = 50\n",
    "imdb_path='D:\\CSV original\\imdb.npz'\n",
    "# Loads the data as lists of integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=imdb_path,num_words=max_features)\n",
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50)\n",
      "(25000, 50)\n",
      "[[2071   56   26 ...   19  178   32]\n",
      " [   2    5    2 ...   16  145   95]\n",
      " [ 215   28  610 ...    7  129  113]\n",
      " ...\n",
      " [   4   65  496 ...    4 3586    2]\n",
      " [  13   18   31 ...   12    9   23]\n",
      " [   2    8 2197 ...  204  131    9]]\n",
      "[2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16    2   19  178   32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2071,   56,   26,  141,    6,  194,    2,   18,    4,  226,   22,\n",
       "         21,  134,  476,   26,  480,    5,  144,   30,    2,   18,   51,\n",
       "         36,   28,  224,   92,   25,  104,    4,  226,   65,   16,   38,\n",
       "       1334,   88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,\n",
       "         15,   16,    2,   19,  178,   32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing.sequence.pad_sequences:\n",
    "# Turns the lists of integers into a 2D integer tensor of shape (samples, max_len)\n",
    "#   This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). \n",
    "# num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise\n",
    "# x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test =keras.utils.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "#\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_train)\n",
    "print(x_train[0])\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16    2   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>\n",
    "<p><span style=\"color: #ffff99;\">The network will learn 8-dimensional embeddings for each of the 10,000 words(5000 words I think), turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classificationNumpy arrays:&nbsp;<code>(x_train, y_train), (x_test, y_test)</code>.</span></p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><span style=\"color: #99ccff;\"><code>kernel_regularizer</code>: Regularizer to apply a penalty on the layer's kernel</span></li>\n",
    "<li style=\"list-style-type: none;\"></li>\n",
    "<li><span style=\"color: #99ccff;\"><code>bias_regularizer</code>: Regularizer to apply a penalty on the layer's bias</span></li>\n",
    "<li style=\"list-style-type: none;\"></li>\n",
    "<li><span style=\"color: #99ccff;\"><code>activity_regularizer</code>: Regularizer to apply a penalty on the layer's output</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 8)             40000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,401\n",
      "Trainable params: 40,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dropout, Dense\n",
    "from keras import regularizers\n",
    "\n",
    "regularizer = regularizers.l2(l2=1e-5)  #A regularizer that applies a L2 regularization penalty \n",
    "                                        #l2: Float; L2 regularization factor which is multiplied by sum squared of weights\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Specifies the maximum input length to the Embedding layer so you can later\n",
    "# flatten the embedded inputs. After the Embedding layer, the activations\n",
    "# have shape (samples, max_len, 8).     #samples may be the batches\n",
    "model.add(Embedding(max_features, 8, input_length=max_len))\n",
    "\n",
    "# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, max_len * 8)\n",
    "model.add(Flatten())\n",
    "\n",
    "# \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Adds the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizer))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #00ccff;\">40000=5000(num of all frequently occured words)*8</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 0.6436 - acc: 0.6388 - val_loss: 0.5213 - val_acc: 0.7820\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4544 - acc: 0.7972 - val_loss: 0.4174 - val_acc: 0.8129\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3856 - acc: 0.8300 - val_loss: 0.3953 - val_acc: 0.8222\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3522 - acc: 0.8450 - val_loss: 0.3929 - val_acc: 0.8218\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3318 - acc: 0.8571 - val_loss: 0.3943 - val_acc: 0.8202\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3150 - acc: 0.8650 - val_loss: 0.4004 - val_acc: 0.8186\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2992 - acc: 0.8726 - val_loss: 0.4038 - val_acc: 0.8180\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2897 - acc: 0.8793 - val_loss: 0.4152 - val_acc: 0.8116\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2795 - acc: 0.8854 - val_loss: 0.4176 - val_acc: 0.8123\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2701 - acc: 0.8891 - val_loss: 0.4233 - val_acc: 0.8119\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2591 - acc: 0.8936 - val_loss: 0.4312 - val_acc: 0.8110\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2569 - acc: 0.8939 - val_loss: 0.4381 - val_acc: 0.8102\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2466 - acc: 0.8977 - val_loss: 0.4447 - val_acc: 0.8083\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2419 - acc: 0.9019 - val_loss: 0.4518 - val_acc: 0.8074\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2376 - acc: 0.9001 - val_loss: 0.4574 - val_acc: 0.8052\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2333 - acc: 0.9052 - val_loss: 0.4645 - val_acc: 0.8033\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2307 - acc: 0.9054 - val_loss: 0.4696 - val_acc: 0.8017\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2255 - acc: 0.9065 - val_loss: 0.4746 - val_acc: 0.8039\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2182 - acc: 0.9114 - val_loss: 0.4806 - val_acc: 0.8006\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2214 - acc: 0.9102 - val_loss: 0.4876 - val_acc: 0.8001\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2159 - acc: 0.9116 - val_loss: 0.4895 - val_acc: 0.7997\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2084 - acc: 0.9159 - val_loss: 0.4981 - val_acc: 0.7987\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2087 - acc: 0.9148 - val_loss: 0.5006 - val_acc: 0.7974\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2096 - acc: 0.9143 - val_loss: 0.5033 - val_acc: 0.7984\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2020 - acc: 0.9179 - val_loss: 0.5112 - val_acc: 0.7975\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2025 - acc: 0.9171 - val_loss: 0.5137 - val_acc: 0.7957\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2029 - acc: 0.9160 - val_loss: 0.5182 - val_acc: 0.7954\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1976 - acc: 0.9212 - val_loss: 0.5241 - val_acc: 0.7930\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2045 - acc: 0.9161 - val_loss: 0.5264 - val_acc: 0.7929\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1987 - acc: 0.9182 - val_loss: 0.5283 - val_acc: 0.7937\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1969 - acc: 0.9200 - val_loss: 0.5319 - val_acc: 0.7944\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1911 - acc: 0.9223 - val_loss: 0.5349 - val_acc: 0.7940\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1929 - acc: 0.9220 - val_loss: 0.5379 - val_acc: 0.7944\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1894 - acc: 0.9239 - val_loss: 0.5441 - val_acc: 0.7928\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1928 - acc: 0.9213 - val_loss: 0.5452 - val_acc: 0.7940\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1850 - acc: 0.9246 - val_loss: 0.5487 - val_acc: 0.7939\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1891 - acc: 0.9213 - val_loss: 0.5520 - val_acc: 0.7927\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1864 - acc: 0.9239 - val_loss: 0.5570 - val_acc: 0.7913\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1825 - acc: 0.9270 - val_loss: 0.5584 - val_acc: 0.7920\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1839 - acc: 0.9261 - val_loss: 0.5594 - val_acc: 0.7928\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1795 - acc: 0.9281 - val_loss: 0.5635 - val_acc: 0.7929\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1840 - acc: 0.9244 - val_loss: 0.5626 - val_acc: 0.7917\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1822 - acc: 0.9263 - val_loss: 0.5668 - val_acc: 0.7905\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.1759 - acc: 0.9266 - val_loss: 0.5694 - val_acc: 0.7902\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1748 - acc: 0.9284 - val_loss: 0.5716 - val_acc: 0.7891\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1795 - acc: 0.9285 - val_loss: 0.5760 - val_acc: 0.7905\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1741 - acc: 0.9291 - val_loss: 0.5769 - val_acc: 0.7901\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1783 - acc: 0.9269 - val_loss: 0.5776 - val_acc: 0.7906\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1746 - acc: 0.9288 - val_loss: 0.5831 - val_acc: 0.7880\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.1793 - acc: 0.9270 - val_loss: 0.5830 - val_acc: 0.7891\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color: #ffcc99;\">You get to a validation accuracy of ~80%, which is pretty good considering that you&rsquo;re only looking at the first 40 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both &ldquo;this movie is a bomb&rdquo; and &ldquo;this movie is the bomb&rdquo; as being negative reviews). It&rsquo;s much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x1f489b982b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01292851, -0.02741463,  0.01828118, ..., -0.02984082,\n",
       "        -0.01374053,  0.03701801],\n",
       "       [-0.03744171,  0.20749305,  0.0353387 , ..., -0.02239804,\n",
       "        -0.27819154, -0.2838543 ],\n",
       "       [ 0.01137996, -0.01576446, -0.00433339, ..., -0.01342939,\n",
       "         0.00468019, -0.00202242],\n",
       "       ...,\n",
       "       [-0.13871156, -0.20225592,  0.43030393, ...,  0.39271358,\n",
       "        -0.13846624, -0.4254482 ],\n",
       "       [ 0.13126855,  0.400036  , -0.4124019 , ..., -0.51967186,\n",
       "         0.00446057,  0.09763086],\n",
       "       [ 0.6401965 , -0.588739  , -0.5614418 , ...,  0.19014515,\n",
       "         0.58151495,  0.3470655 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "print(weights.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
